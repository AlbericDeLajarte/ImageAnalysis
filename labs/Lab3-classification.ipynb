{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IAPR 2020:][iapr2020] Lab 3 ‒  Classification\n",
    "\n",
    "**Author:** Quentin Talon, Albéric De Lajarte\n",
    "**Due date:** 08.05.2020\n",
    "\n",
    "[iapr2018]: https://github.com/LTS5/iapr-2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract relevant data\n",
    "We first need to extract the `lab-03-data.tar.gz` archive.\n",
    "To this end, we use the [tarfile] module from the Python standard library.\n",
    "\n",
    "[tarfile]: https://docs.python.org/3.6/library/tarfile.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "data_base_path = os.path.join(os.pardir, 'data')\n",
    "data_folder = 'lab-03-data'\n",
    "tar_path = os.path.join(data_base_path, data_folder + '.tar.gz')\n",
    "with tarfile.open(tar_path, mode='r:gz') as tar:\n",
    "    tar.extractall(path=data_base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "In this part, we will study classification based on the data available in the Matlab file `classification.mat` that you will under `lab-03-data/part1`.\n",
    "There are 3 data sets in this file, each one being a training set for a given class.\n",
    "They are contained in variables `a`, `b` and `c`.\n",
    "\n",
    "**Note**: we can load Matlab files using the [scipy.io] module.\n",
    "\n",
    "[scipy.io]: https://docs.scipy.org/doc/scipy/reference/io.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "data_part1_path = os.path.join(data_base_path, data_folder, 'part1', 'classification.mat')\n",
    "matfile = scipy.io.loadmat(data_part1_path)\n",
    "a = matfile['a']\n",
    "b = matfile['b']\n",
    "c = matfile['c']\n",
    "\n",
    "print(a.shape, b.shape, c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Dataset visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from numpy import linalg as LA\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "ax = plt.gca()\n",
    "\n",
    "#Plot dataset\n",
    "plt.scatter(a[:,0], a[:,1], marker= '+')\n",
    "plt.scatter(b[:,0], b[:,1], marker= 'o')\n",
    "plt.scatter(c[:,0], c[:,1], marker= ',')\n",
    "\n",
    "#Compute mean of each dataset\n",
    "mu_a = [np.mean(a[:,0]), np.mean(a[:,1])]\n",
    "mu_b = [np.mean(b[:,0]), np.mean(b[:,1])]\n",
    "mu_c = [np.mean(c[:,0]), np.mean(c[:,1])]\n",
    "\n",
    "#Compute variance of each dataset\n",
    "var_a = np.cov(a, rowvar=False)\n",
    "var_b = np.cov(b, rowvar=False)\n",
    "var_c = np.cov(c, rowvar=False)\n",
    "\n",
    "#Plot dataset\n",
    "plt.scatter(mu_a[0], mu_a[1], marker= '+', color = 'r')\n",
    "plt.scatter(mu_b[0], mu_b[1], marker= 'o', color = 'r')\n",
    "plt.scatter(mu_c[0], mu_c[1], marker= ',', color = 'r')\n",
    "\n",
    "#Plot covariance ellipse\n",
    "w,v = LA.eig(var_a)\n",
    "ax.add_patch(Ellipse(xy=mu_a, width=4*np.sqrt(w[0]), height=4*np.sqrt(w[1]), edgecolor='r', fc='None', lw=2, angle=np.degrees(np.arctan2(w[1], w[0])) ))\n",
    "w,v = LA.eig(var_b)\n",
    "ax.add_patch(Ellipse(xy=mu_b, width=4*np.sqrt(w[0]), height=4*np.sqrt(w[1]), edgecolor='r', fc='None', lw=2, angle=np.degrees(np.arctan2(w[1], w[0])) ))\n",
    "w,v = LA.eig(var_c)\n",
    "ax.add_patch(Ellipse(xy=mu_c, width=4*np.sqrt(w[0]), height=4*np.sqrt(w[1]), edgecolor='r', fc='None', lw=2, angle=np.degrees(np.arctan2(w[0], w[1])) ))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Bayes method\n",
    "Using the Bayes method, give the analytical expression of the separation curves between those three classes.\n",
    "Do reasonable hypotheses about the distributions of those classes and estimate the corresponding parameters based on the given training sets.\n",
    "Draw those curves on a plot, together with the training data.\n",
    "For simplicity reasons, round the estimated parameters to the closest integer value.\n",
    "\n",
    "*Add your implementation and discussion*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analytical expression\n",
    "We compute $g_{ij}$ between all the classes. Then, for a given point $x$ we compute where it belongs. We proceed by elemination as in this example :\n",
    "- A or B -> $x\\notin B$\n",
    "- A or C -> $x\\notin C$\n",
    "- B or C -> $x\\notin C$\n",
    "\n",
    "If a point doesnt not belong to B and C, we conclue it's in A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical expression of Bayesian discriminant function with covariance matrix and no constant \n",
    "def discriminant_bayesian_complete(mu, var, XX, YY):\n",
    "    def foo(X, mu, var):\n",
    "        mu = np.asmatrix(mu).T\n",
    "        var = np.asmatrix(var)\n",
    "        g = -0.5*(X.T@var.I@X) + 0.5*X.T@var.I@mu - 0.5*mu.T@var.I@mu + 0.5*mu.T@var.I@X - np.log(2*np.pi*np.sqrt(LA.norm(var)))\n",
    "        return g\n",
    "    G = np.zeros((len(YY[:,0]), len(XX[0,:])))\n",
    "    for i, x in enumerate(XX[0,:]):\n",
    "        for j, y in enumerate(YY[:,0]):\n",
    "            X = np.matrix([[x], [y]])\n",
    "            #print(X)\n",
    "            G[j,i] = foo(X, mu, var)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical expression of Bayesian discriminant function with diagonal covariance matrix and no constant \n",
    "def discriminant_bayesian_diagonal(mu, var, XX, YY):\n",
    "    C = np.transpose(mu)@LA.inv(var)@mu/(-2)\n",
    "    G = (var[1,1]*XX**2 + var[0,0]*YY**2)/(-2*var[1,1]*var[0,0]) + (var[1,1]*XX*mu[0] + var[0,0]*YY*mu[1])/(var[1,1]*var[0,0]) + C\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create 2D space\n",
    "XX, YY = np.meshgrid(np.linspace(-20, 16, 100), np.linspace(-8, 8, 100))\n",
    "\n",
    "G_a = {\"name\":\"G_a\",\n",
    "       \"value_diagonal\":discriminant_bayesian_diagonal(mu_a, var_a, XX, YY),\n",
    "       \"value_complete\":discriminant_bayesian_complete(mu_a, var_a, XX, YY),\n",
    "       \"var\":(var_a),\n",
    "       \"mu\":(mu_a),\n",
    "       \"datapoint\":a,\n",
    "       \"marker\":\"+\",\n",
    "       \"color\":'r'}\n",
    "G_b = {\"name\":\"G_b\",\n",
    "       \"value_diagonal\":discriminant_bayesian_diagonal(mu_b, var_b, XX, YY),\n",
    "       \"value_complete\":discriminant_bayesian_complete(mu_b, var_a, XX, YY),\n",
    "       \"var\":(var_b),\n",
    "       \"mu\":(mu_b),\n",
    "       \"datapoint\":b,\n",
    "       \"marker\":\"o\",\n",
    "       \"color\":\"g\"}\n",
    "G_c = {\"name\":\"G_c\",\n",
    "       \"value_diagonal\":discriminant_bayesian_diagonal(mu_c, var_c, XX, YY),\n",
    "       \"value_complete\":discriminant_bayesian_complete(mu_c, var_c, XX, YY),\n",
    "       \"var\":(var_c),\n",
    "       \"mu\":(mu_c),\n",
    "       \"datapoint\":c,\n",
    "       \"marker\":\",\",\n",
    "       \"color\":\"b\"}\n",
    "Gs = [G_a, G_b, G_c]\n",
    "\n",
    "#Compute the zone that belogs to class\n",
    "for v, b in zip([\"value_diagonal\", \"value_complete\"], [\"bool_zone_diagonal\", \"bool_zone_complete\"]):\n",
    "    G_a_b = G_a[v]-G_b[v]>0\n",
    "    G_a_c = G_a[v]-G_c[v]>0\n",
    "    G_b_c = G_b[v]-G_c[v]>0\n",
    "    G_a[b] = np.logical_and(G_a_b, G_a_c)\n",
    "    G_b[b] = np.logical_and(np.logical_not(G_a_b), G_b_c)\n",
    "    G_c[b] = np.logical_and(np.logical_not(G_a_c), np.logical_not(G_b_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the function with the dataset\n",
    "fig, axes = plt.subplots(4, len(Gs), figsize = (15, 20), sharey=True, sharex=True)\n",
    "for i, v in enumerate([\"value_diagonal\", \"value_complete\", \"bool_zone_diagonal\", \"bool_zone_complete\"]):\n",
    "    for j, (G, axe) in enumerate(zip(Gs, axes[i,:])):\n",
    "        if j == 0: axe.set_ylabel(v)\n",
    "        axe.pcolor(XX,YY, G[v])\n",
    "        axe.scatter(G[\"datapoint\"][:,0], G[\"datapoint\"][:,1], marker = G[\"marker\"])\n",
    "        if i == 0: axe.set_title(G[\"name\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis** : On the plots above, we compared a simplified version that contains only diagonal terms in the covariance matrix to the full covariant matrix.  \n",
    "The zone in yellow correspond to the zoning to to corresponding class. We see few differences between the *full* and the *diagonal* version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Albéric, je sais pas trop ce que tu voulais faire ici :/ Ca run mais je suis pas sur que ce soit correct ce que j'ai changé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find contour of near zero term\n",
    "from skimage.measure import find_contours\n",
    "G_inter = abs(G_a[\"value_complete\"]-G_c[\"value_complete\"])\n",
    "contour = find_contours(G_inter, 54, fully_connected = \"low\")[0]\n",
    "contour = [XX[0,:][contour[:, 1].astype(int)], YY[:,0][contour[:, 0].astype(int)]]\n",
    "\n",
    "# Fit parametric quadratic curve on the contour\n",
    "contour_parameter = np.polyfit(contour[0], contour[1], 2)\n",
    "contour_quad = np.poly1d(contour_parameter)\n",
    "\n",
    "# Plot bayesian function and contour\n",
    "plt.pcolor(XX, YY, G_inter)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.plot(contour[0], contour[1], linewidth = 4)\n",
    "plt.plot(XX[0,:], contour_quad(XX[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test with 1D gaussian function\n",
    "def gaussian(x, mu, sig):\n",
    "    return np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.)))\n",
    "\n",
    "t = np.linspace(0, 10, 100)\n",
    "f1 = gaussian(t, 7, 1)\n",
    "f2 = gaussian(t, 2, 2)\n",
    "\n",
    "# plot 2 gaussian and their difference\n",
    "plt.plot(t, f1)\n",
    "plt.plot(t, f2)\n",
    "plt.plot(t, 0.01/abs(f2-f1))\n",
    "\n",
    "plt.show()\n",
    "# Note: difference works well, but scaling necessary for visualization, and careful with edge of function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Mahalanobis distance\n",
    "For classes `a` and `b`, give the expression of the Mahalanobis distance used to classify a point in class `a` or `b`, and verify the obtained classification, in comparison with the \"complete\" Bayes classification, for a few points of the plane.\n",
    "\n",
    "*Add your implementation and discussion*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a covariance matrix that is the mean of the two matrices. The Mahalanobis distance is computed from the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import mahalanobis\n",
    "from tqdm.notebook import tqdm\n",
    "var_mean = (G_a[\"var\"]+G_b[\"var\"])/2\n",
    "for v in (Gs): #Gs is a list of dictionaries. Each dictionary contains datas of a class (a, b or c)\n",
    "    v[\"mahalanobis\"] = np.asarray([[mahalanobis(np.asarray([x,y]), v[\"mu\"], LA.inv(var_mean)) for x in (XX[0,:])] for y in (YY[:,0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize = (15, 7.5))\n",
    "for ax, G in zip(axes, Gs[0:2]):\n",
    "    ax.pcolor(XX,YY, G[\"mahalanobis\"])\n",
    "    ax.scatter(G[\"datapoint\"][:,0], G[\"datapoint\"][:,1], marker = G[\"marker\"], color = G[\"color\"])\n",
    "    ax.set_title(\"class :{}, mu is at ({:.3f},{:.3f})\".format(G[\"name\"][-1],*G[\"mu\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a distance map with the Mahalanobis distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize = (15, 7.5))\n",
    "G_a_b_mahalanobis = G_a[\"mahalanobis\"]-G_b[\"mahalanobis\"]<0 #Smallest distance -> in class a\n",
    "G_a_b_bayse = G_a[\"value_complete\"]-G_b[\"value_complete\"]>0 #Biggest probability -> in class a\n",
    "for ax, v, t in zip(axes, [G_a_b_mahalanobis, G_a_b_bayse], [\"Mahalanobis distance classifier\", \"Baysian classifier\"]):\n",
    "    ax.pcolor(XX,YY, v)\n",
    "    ax.scatter(G_a[\"datapoint\"][:,0], G_a[\"datapoint\"][:,1], marker = G_a[\"marker\"], color = G_a[\"color\"])\n",
    "    ax.set_title(t)\n",
    "    ax.scatter(G_b[\"datapoint\"][:,0], G_b[\"datapoint\"][:,1], marker = G_b[\"marker\"], color = G_b[\"color\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "In this part, we aim to classify digits using the complete version of MNIST digits dataset.\n",
    "The dataset consists of 60'000 training images and 10'000 test images of handwritten digits.\n",
    "Each image has size 28x28, and has assigned a label from zero to nine, denoting the digits value.\n",
    "Given this data, your task is to construct a Multilayer Perceptron (MLP) for supervised training and classification and evaluate it on the test images.\n",
    "\n",
    "Download the MNIST dataset (all 4 files) from http://yann.lecun.com/exdb/mnist/ under `lab-03-data/part2`.\n",
    "You can then use the script provided below to extract and load training and testing images in Python.\n",
    "\n",
    "To create an MLP you are free to choose any library.\n",
    "In case you don't have any preferences, we encourage you to use the [scikit-learn] package; it is a simple, efficient and free tool for data analysis and machine learning.\n",
    "In this [link][sklearn-example], you can find a basic example to see how to create and train an MLP using [scikit-learn].\n",
    "Your network should have the following properties:\n",
    "* Input `x`: 784-dimensional (i.e. 784 visible units representing the flattened 28x28 pixel images).\n",
    "* 100 hidden units `h`.\n",
    "* 10 output units `y`, i.e. the labels, with a value close to one in the i-th class representing a high probability of the input representing the digit `i`.\n",
    "\n",
    "If you need additional examples you can borrow some code from image classification tutorials.\n",
    "However, we recommend that you construct a minimal version of the network on your own to gain better insights.\n",
    "\n",
    "[scikit-learn]: http://scikit-learn.org/stable/index.html\n",
    "[sklearn-example]: http://scikit-learn.org/stable/modules/neural_networks_supervised.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dataset loading\n",
    "Here we first declare the methods `extract_data` and `extract_labels` so that we can reuse them later in the code.\n",
    "Then we extract both the data and corresponding labels, and plot randomly some images and corresponding labels of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_data(filename, image_shape, image_number):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(np.prod(image_shape) * image_number)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        data = data.reshape(image_number, image_shape[0], image_shape[1])\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_labels(filename, image_number):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * image_number)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (28, 28)\n",
    "train_set_size = 60000\n",
    "test_set_size = 10000\n",
    "\n",
    "data_part2_folder = os.path.join(data_base_path, data_folder, 'part2')\n",
    "\n",
    "train_images_path = os.path.join(data_part2_folder, 'train-images-idx3-ubyte.gz')\n",
    "train_labels_path = os.path.join(data_part2_folder, 'train-labels-idx1-ubyte.gz')\n",
    "test_images_path = os.path.join(data_part2_folder, 't10k-images-idx3-ubyte.gz')\n",
    "test_labels_path = os.path.join(data_part2_folder, 't10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "train_images = extract_data(train_images_path, image_shape, train_set_size)\n",
    "test_images = extract_data(test_images_path, image_shape, test_set_size)\n",
    "train_labels = extract_labels(train_labels_path, train_set_size)\n",
    "test_labels = extract_labels(test_labels_path, test_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prng = np.random.RandomState(seed=123456789)  # seed to always re-draw the same distribution\n",
    "plt_ind = prng.randint(low=0, high=train_set_size, size=10)\n",
    "\n",
    "fig, axes = plt.subplots(1, 10, figsize=(12, 3))\n",
    "for ax, im, lb in zip(axes, train_images[plt_ind], train_labels[plt_ind]):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 MLP\n",
    "*Add your implementation and discussion*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 100\n",
    "train_images_flat = train_images.reshape(train_set_size, -1)\n",
    "test_images_flat = test_images.reshape(test_set_size, -1)\n",
    "print(\"Training: {}, Test: {}\".format(train_images_flat.shape, test_images_flat.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "def accuracy(key, solver):\n",
    "    predicted = np.argmax(solver.predict_proba(test_images_flat), 1)\n",
    "    score =  sum(predicted == test_labels) / test_set_size\n",
    "    print(\"Our score for {} {:.2%} of correct answers\".format(key, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp_lbfgs = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(h,))\n",
    "mlp_sgd = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(h,))\n",
    "mlp_adam = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes= (h,))\n",
    "mlps = {\"lbfgs\":mlp_lbfgs, \"sgd\":mlp_sgd, \"adam\":mlp_adam}\n",
    "for key in tqdm(mlps):\n",
    "    mlps[key].fit(train_images_ravel, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in mlps:\n",
    "    accuracy(key, mlps[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis** : Following the example given in this lab we ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
